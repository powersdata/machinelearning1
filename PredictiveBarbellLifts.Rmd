---
title: "Predict Quality Class for Barbell Lifts"
author: "Ted Powers"
date: "Jan 21, 2015"
output: html_document
---
        
###Summary
Accelerometers were used to capture 6 participants doing Unilateral Dumbbell Biceps Curls.  Participants were ask to perform the curls in 5 different ways or classes.  Our model was created using the data from the accelerometers on the belt, forearm, arm, and dumbell.  We are trying to predict the manner in which the curl was performed.

The following packages were used to obtain the predicted results along with the random seed set to 42.

```{r, echo=T, message=FALSE, warning=FALSE, cache = FALSE, results='markup'}
library(caret); library(rattle); library(gbm)
set.seed(42)
```

###Data
The training data for this project are available here: <https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv>

The test data are available here: <https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv>

More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). 

```{r, echo=F, message=FALSE, warning=FALSE, cache = FALSE, results="hide"}
setwd("~/GitHub/machinelearning1")
if (!dir.exists("data")) { dir.create("data")}

trainUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
trainFile <- "train.csv"
trainFilePath <- paste(getwd(),"data", trainFile, sep = "/")

testUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
testFile <- "test.csv"
testFilePath <- paste(getwd(),"data", testFile, sep = "/")

## Download Files from internet only if not in data directory
if (!file.exists(paste(getwd(), "data", "train.csv", sep ="/"))) {
        setInternet2(use = TRUE)
        download.file(trainUrl, destfile = trainFilePath)
}

if (!file.exists(paste(getwd(), "data", "test.csv", sep ="/"))) {
        setInternet2(use = TRUE)
        download.file(testUrl, destfile = testFilePath)
}

## load the file from disk only if it 
## hasn't already been read into a variable
if(!(exists("raw.training"))) {
        raw.training <- read.csv(trainFilePath, na.strings = c("NA","#DIV/0!",""))
}
if(!(exists("test"))) {
        test <- read.csv(testFilePath, na.strings = c("NA","#DIV/0!",""))
}
training <- raw.training
```

###Cleaning the Training-Set
Included in the dataset are featured columns that the authors used to characterize the performance classes.  These columns of data features the max/min value, average, skewness, etc. of the performance class and are not relevant to include as a predictor.  In addition to removing these featured columns, I looked but did not find columns that had near zero variance or lots of missing data.  As a result, the number of predictors for 'classe' was reduced to 52.

While this reduction may reduce our accuracy for our testing set by allowing more in-sample errors, it has several added benefits.

- The processing speed and computational requirements are reduced.
- Reduces likelihood of overfitting the model to the training data and thus insuring that the out-of-sample errors are more accurate.

```{r, echo=T, message=FALSE, warning=FALSE, cache = FALSE, results="markup"}
# Remove all feature columns
featured.cols <- grep("^max|^min|^ampl|^var|^avg|^stdd|^ske|^kurt", names(raw.training))
training <- training[-featured.cols]

#Removed variables related with data acquisition (like: id, timestamps, individuals’ names, etc.)
training <- training[, -(1:7)]   

#Removed all variables that had little variance
nearzero <- nearZeroVar(training, saveMetrics = TRUE)
training <- training[, !nearzero$nzv]

#Removed all variables that had more than 30% of the data missing
toberem <-sapply(colnames(training), function(x)
        if (sum(is.na(training[, x])) > 0.30 * nrow(training))
        {return(TRUE)}
        else{return(FALSE)})
training <- training[,!toberem]
```

###Partioning the training set into two
The training data is partitioned into two data sets, 75% for training, 25% for testing:
        
```{r, echo=T, message=FALSE, warning=FALSE, cache = FALSE, results="markup"}
inTrain <- createDataPartition(y=training$classe, p=.75, list=FALSE)
testing <- training[-inTrain, ]
training <- training[inTrain, ]
```

###Model Preprocessing and Cross Validation
All the predictors were plotted to identify any extreme values. An example is shown below. 9 row values were removed from the dataset.

```{r, echo=T, message=FALSE, warning=FALSE, cache = FALSE, fig.width=6, fig.height=3.5, fig.align='center', results="markup"}
# Explore plots to determine outliers
par(mfrow = c(1, 2))
for (i in 1:(ncol(training)-1)) {

    if (names(training[i]) == "accel_belt_y") {
        hist(training[[i]], xlab = names(training[i]), main = names(training[i]))
        plot(training[[i]], main = names(training[i]))
     }
}

#Outliers values are removed
out.rows <- training$accel_belt_y > 100 | training$accel_belt_y < -50 |
        training$total_accel_dumbbell > 50 | training$gyros_dumbbell_x < -50 |
        training$gyros_dumbbell_y > 10 | training$gyros_dumbbell_z  > 10 |
        training$magnet_dumbbell_y < -2000 | training$total_accel_forearm > 90 |
        training$gyros_forearm_x < -10 | training$gyros_forearm_y > 50 |
        training$gyros_forearm_z > 50 | training$accel_forearm_y > 800
training <- training[!out.rows, ]
```

The number of variables is reduced to 26 by using Principle Component Analysis (PCA).  PCA is used to avoid overfitting by combining highly correlated fields as well as reduce the computational requirements.

```{r, echo=F, message=FALSE, warning=FALSE, cache = TRUE, results="markup"}
#Shows how much pre-processing reduces the number of variables
preObj <- preProcess(x = training, method = "pca")
print(preObj)
```

The introduction of a training control process performs the PCA reduction and adds a 7 fold cross validation. 7-fold cross validation avoids overfitting by breaking the training set into 7 subsets.

```{r, echo=T, message=FALSE, warning=FALSE, cache = TRUE, results="markup"}
tc <- trainControl(method = "cv", number = 7, verboseIter=FALSE , preProcOptions="pca", allowParallel=TRUE)
```

```{r, echo=F, message=FALSE, warning=FALSE, cache = TRUE, results="hide"}
rm(testFile, testFilePath, testUrl, trainFile, trainFilePath, trainUrl, preObj, i, 
   out.rows, inTrain, nearzero, toberem, featured.cols)
```

###Model Selection
Three base models to cover a wide variety of possible distributions are used: Random Forest, Gradient Boosting, and Linear Discriminated Analysis model. 

```{r, echo=T, message=FALSE, warning=FALSE, cache = TRUE, results="hide"}
rf.model <- suppressMessages(train(classe ~ ., data = training, method = "rf", trControl= tc))
gbm.model <- suppressMessages(train(classe ~., data = training, method="gbm", trControl = tc))
lda.model <- suppressMessages(train(classe ~ ., data = training, method = "lda", trControl= tc))
```

```{r, echo=F, message=FALSE, warning=FALSE, cache = FALSE, results="hide"}
model <- c("Random Forest", "Gradient Boosting", "Linear Discriminate Analysis" )
Best_Accuracy <- c(
        max(rf.model$results$Accuracy),
        max(gbm.model$results$Accuracy),
        max(lda.model$results$Accuracy)
        )
Worst_Accuracy_SD <- c(
        max(rf.model$results$AccuracySD),
        max(gbm.model$results$AccuracySD),
        max(lda.model$results$AccuracySD)
        )
Worst_Accuracy <- c(
        min(rf.model$results$Accuracy),
        min(gbm.model$results$Accuracy),
        min(lda.model$results$Accuracy)
        )
Best_Accuracy_SD <- c(
        min(rf.model$results$AccuracySD),
        min(gbm.model$results$AccuracySD),
        min(lda.model$results$AccuracySD)
        )
```

Accuracy comparision and out of sample errors:
```{r, echo=F, message=FALSE, warning=FALSE, cache = FALSE, results="asis"}
performance <- cbind(model, Best_Accuracy, Best_Accuracy_SD, Worst_Accuracy, Worst_Accuracy_SD)
knitr::kable(performance)
```

Best Model:
```{r, echo=F, message=FALSE, warning=FALSE, cache = TRUE, results="markup"}
rf.model
```

The random forest model method was the best model with the average accuracy of each of the 7 folds greater than 98.5%.  In addition, the out-of-sample error was very low as the worst standard deviation average of the 7 folds was less than .35%.  

###Prediction of “classe” variable for the test set

```{r, echo=T, message=FALSE, warning=FALSE, cache = TRUE, results="hide"}
rf.Pred <- predict(rf.model, testing)
```

Checking our Predictions with confusionMatrix:

```{r, echo=T, message=FALSE, warning=FALSE, cache = TRUE, results="markup"}
confusionMatrix(rf.Pred, testing$classe)
```

##Conclusions
```{r, echo=F, message=FALSE, warning=FALSE, cache = TRUE, results="hide"}
rf.Pred.test <- predict(rf.model, test)
```
The random forest model provides an outstanding accuracy and, accordingly, the predictions for the test set were correct in 100% of the cases.
