---
title: "Predictive Barbell Lifts"
author: "Ted Powers"
date: "Jan 21, 2015"
output: html_document
---
        
###Summary
Accelerometers were used to capture 6 participants doing Unilateral Dumbbell Biceps Curls.  Participants were ask to perform the curls in 5 different ways or classes.  Our model was created using the data from the accelerometers on the belt, forearm, arm, and dumbell.  We are trying to predict the manner in which the curl was performed.

The following packages were used to obtain the predicted results along with the random seed set to 42.

```{r, echo=T, message=FALSE, warning=FALSE, cache = FALSE, results='markup'}
#detach(package:igraph, unload=TRUE)#library(klaR)
#library(abind)
#library(arm)

#library(MASS)
library(caret)
#library(ggplot2)
#library(randomForest)
library(rattle)
library(gbm)
#detach("package:ggplot2", unload=TRUE)
#library(kernlab)


#library()

set.seed(42)
```

###Data
The training data for this project are available here: 
        
<https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv>

The test data are available here: 
        
<https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv>

More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). 

```{r, echo=F, message=FALSE, warning=FALSE, cache = FALSE, results="hide"}
setwd("~/GitHub/machinelearning1")
if (!dir.exists("data")) { dir.create("data")}

trainUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
trainFile <- "train.csv"
trainFilePath <- paste(getwd(),"data", trainFile, sep = "/")

testUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
testFile <- "test.csv"
testFilePath <- paste(getwd(),"data", testFile, sep = "/")

## Download Files from internet only if not in data directory
if (!file.exists(paste(getwd(), "data", "train.csv", sep ="/"))) {
        setInternet2(use = TRUE)
        download.file(trainUrl, destfile = trainFilePath)
}

if (!file.exists(paste(getwd(), "data", "test.csv", sep ="/"))) {
        setInternet2(use = TRUE)
        download.file(testUrl, destfile = testFilePath)
}

## load the file from disk only if it 
## hasn't already been read into a variable
if(!(exists("training"))) {
        training <- read.csv(trainFilePath, na.strings = c("NA","#DIV/0!",""))
}
if(!(exists("test"))) {
        test <- read.csv(testFilePath, na.strings = c("NA","#DIV/0!",""))
}

rm(testFile, testFilePath, testUrl, trainFile, trainFilePath, trainUrl)
```

###Cleaning the Training-Set
The original dataset contained 160 variables which was reduced to 53 and even further by preprocessing using PCA referred to later.  While this reduction may reduce our accuracy for our testing set by allowing more in-sample errors, it has several added benefits.

- The processing speed and computational requirements are reduced.
- Reduces likelihood of overfitting the model to the training data and thus insuring that the out-of-sample errors are more accurate.

```{r, echo=T, message=FALSE, warning=FALSE, cache = FALSE, results="markup"}
#Removed all variables that had little variance
nearzero <- nearZeroVar(training, saveMetrics = TRUE)
training <- training[, !nearzero$nzv]

#Removed all variables that had more than 30% of the data missing
toberem <-sapply(colnames(training), function(x)
        if (sum(is.na(training[, x])) > 0.30 * nrow(training))
        {return(TRUE)}
        else{return(FALSE)})
        training <- training[,!toberem]
        
#Removed variables related with data acquisition (like: id, timestamps, individuals’ names, etc.)
training <- training[, -(1:6)]   
rm(nearzero, toberem)
```



###Partioning the training set into two
The training data is partitioned into two data sets, 75% for training, 25% for testing:
        
```{r, echo=T, message=FALSE, warning=FALSE, cache = FALSE, results="markup"}
inTrain <- createDataPartition(y=training$classe, p=.75, list=FALSE)
testing <- training[-inTrain, ]
training <- training[inTrain, ]
rm(inTrain)
```

All the predictors were plotted to identify any extreme value. Raw plots highlighted few cases of abnormal values. An example is shown below. These values were removed from the dataset.
```{r, echo=T, message=FALSE, warning=FALSE, cache = FALSE, results="markup"}
# Explore histos. Do we need additional transformations?
par(mfrow = c(3, 5))
#for (i in 1:(ncol(training)-1)) {
for (i in 1:15) {
#    if (names(training[i]) == "roll_belt") {
        #hist(training[[i]], xlab = names(training[i]), main = names(training[i]))
        plot(training[[i]], main = names(training[i]))
        #qqnorm(training[[i]], main=names(training[i]))
    }
}
```

###Model Specification and Cross Validation
I selected three base models to cover a wide variety of possible distributions: Random Forest, Gradient Boosting, and Linear Discriminated Analysis model.  I also introduced a training control process to the three models as I wanted to see the effects of pre-processing the data to prevent overfitting the data:

- Principle Component Analysis (PCA) is used in the pre-processing in order to avoid overfitting by combining highly correlated fields
- 7-fold cross validation to limit avoid overfitting by breaking the training set into 7 subsets

```{r, echo=T, message=FALSE, warning=FALSE, cache = TRUE, results="hide"}
tc <- trainControl(method = "cv", number = 7, verboseIter=FALSE , preProcOptions="pca", allowParallel=TRUE)

rf.model <- suppressMessages(train(classe ~ ., data = training, method = "rf"))
rf.cv.model <- suppressMessages(train(classe ~ ., data = training, method = "rf", trControl= tc))
gbm.model <- suppressMessages(train(classe ~., data = training, method="gbm"))
gbm.cv.model <- suppressMessages(train(classe ~., data = training, method="gbm", trControl = tc))
lda.model <- suppressMessages(train(classe ~ ., data = training, method = "lda"))
lda.cv.model <- suppressMessages(train(classe ~ ., data = training, method = "lda", trControl= tc))
```

Accuracy comparision

```{r, echo=F, message=FALSE, warning=FALSE, cache = FALSE, results="asis"}
model <- c("Random Forest",  "Random Forest with CV", "Gradient Boosting", "Gradient Boosting with CV",
           "Linear Discriminate Analysis", "Linear Discriminate Analysis with CV" )
Accuracy <- c(
        max(rf.model$results$Accuracy),
        max(rf.cv.model$results$Accuracy),
        max(gbm.model$results$Accuracy),
        max(gbm.cv.model$results$Accuracy),
        max(lda.model$results$Accuracy),
        max(lda.cv.model$results$Accuracy)
        )
        
performance <- cbind(model,Accuracy)
knitr::kable(performance)
```

Random forest provides the best results and will provide the predictions for the submission. Even if the Out of sample error cannot be estimated exactly, the in-sample error obtained through cross-validation is calculated over different test sets and should provide a better estimate of out-of sample error with respect to the case of no cross-validation.

##Prediction of “classe” variable for the test set

```{r, echo=F, message=FALSE, warning=FALSE, cache = TRUE, results="hide"}
#rf.Pred <- predict(rf.model, testing)
rf.cv.Pred <- predict(rf.cv.model, testing)
gbm.cv.Pred <- predict(gbm.cv.model, testing)
```

Checking if the models give same predictions

```{r, echo=F, message=FALSE, warning=FALSE, cache = TRUE, results="hide"}
#confusionMatrix(rf.Pred, testing$classe)
confusionMatrix(rf.cv.Pred, testing$classe)
#confusionMatrix(gbm.cv.Pred, testing$classe)

#prediction <- data.frame(cbind(rf.cv.Pred, gbm.cv.Pred))
#prediction$same <- with(prediction, rf.cv.Pred == gbm.cv.Pred)
#colnames(prediction) <- c("Random Forest", "Gradient Boosting", "Same Prediction")
#knitr::kable(prediction)
```

Generation of the files to be submitted is made through the provided function

```{r, echo=F, message=FALSE, warning=FALSE, cache = TRUE, results="hide"}
pml_write_files = function(x){
        n = length(x)
        for(i in 1:n){
                filename = paste0("problem_id_",i,".txt")
                write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
        }
}

pml_write_files(rf.cv.Pred)
#pml_write_files(svmrPred)
```

##Conclusions
The random forest model provides an outstanding accuracy and, accordingly, the predictions for the test set were correct in 100% of the cases.
