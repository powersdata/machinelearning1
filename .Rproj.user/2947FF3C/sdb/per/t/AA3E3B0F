{
    "contents" : "---\ntitle: \"Predict Quality Class for Barbell Lifts\"\nauthor: \"Ted Powers\"\ndate: \"Jan 21, 2015\"\noutput: html_document\n---\n        \n###Summary\nAccelerometers were used to capture 6 participants doing Unilateral Dumbbell Biceps Curls.  Participants were ask to perform the curls in 5 different ways or classes.  Our model was created using the data from the accelerometers on the belt, forearm, arm, and dumbell.  We are trying to predict the manner in which the curl was performed.\n\nThe following packages were used to obtain the predicted results along with the random seed set to 42.\n\n```{r, echo=T, message=FALSE, warning=FALSE, cache = FALSE, results='markup'}\nlibrary(caret); library(rattle); library(gbm)\nset.seed(42)\n```\n\n###Data\nThe training data for this project are available here: <https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv>\n\nThe test data are available here: <https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv>\n\nMore information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). \n\n```{r, echo=F, message=FALSE, warning=FALSE, cache = FALSE, results=\"hide\"}\nsetwd(\"~/GitHub/machinelearning1\")\nif (!dir.exists(\"data\")) { dir.create(\"data\")}\n\ntrainUrl <- \"http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\"\ntrainFile <- \"train.csv\"\ntrainFilePath <- paste(getwd(),\"data\", trainFile, sep = \"/\")\n\ntestUrl <- \"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\"\ntestFile <- \"test.csv\"\ntestFilePath <- paste(getwd(),\"data\", testFile, sep = \"/\")\n\n## Download Files from internet only if not in data directory\nif (!file.exists(paste(getwd(), \"data\", \"train.csv\", sep =\"/\"))) {\n        setInternet2(use = TRUE)\n        download.file(trainUrl, destfile = trainFilePath)\n}\n\nif (!file.exists(paste(getwd(), \"data\", \"test.csv\", sep =\"/\"))) {\n        setInternet2(use = TRUE)\n        download.file(testUrl, destfile = testFilePath)\n}\n\n## load the file from disk only if it \n## hasn't already been read into a variable\nif(!(exists(\"raw.training\"))) {\n        raw.training <- read.csv(trainFilePath, na.strings = c(\"NA\",\"#DIV/0!\",\"\"))\n}\nif(!(exists(\"test\"))) {\n        test <- read.csv(testFilePath, na.strings = c(\"NA\",\"#DIV/0!\",\"\"))\n}\ntraining <- raw.training\n```\n\n###Cleaning the Training-Set\nIncluded in the dataset are featured columns that the authors used to characterize the performance classes.  These columns of data features the max/min value, average, skewness, etc. of the performance class and are not relevant to include as a predictor.  In addition to removing these featured columns, I looked but did not find columns that had near zero variance or lots of missing data.  As a result, the number of predictors for 'classe' was reduced to 52.\n\nWhile this reduction may reduce our accuracy for our testing set by allowing more in-sample errors, it has several added benefits.\n\n- The processing speed and computational requirements are reduced.\n- Reduces likelihood of overfitting the model to the training data and thus insuring that the out-of-sample errors are more accurate.\n\n```{r, echo=T, message=FALSE, warning=FALSE, cache = FALSE, results=\"markup\"}\n# Remove all feature columns\nfeatured.cols <- grep(\"^max|^min|^ampl|^var|^avg|^stdd|^ske|^kurt\", names(raw.training))\ntraining <- training[-featured.cols]\n\n#Removed variables related with data acquisition (like: id, timestamps, individuals’ names, etc.)\ntraining <- training[, -(1:7)]   \n\n#Removed all variables that had little variance\nnearzero <- nearZeroVar(training, saveMetrics = TRUE)\ntraining <- training[, !nearzero$nzv]\n\n#Removed all variables that had more than 30% of the data missing\ntoberem <-sapply(colnames(training), function(x)\n        if (sum(is.na(training[, x])) > 0.30 * nrow(training))\n        {return(TRUE)}\n        else{return(FALSE)})\ntraining <- training[,!toberem]\n```\n\n###Partioning the training set into two\nThe training data is partitioned into two data sets, 75% for training, 25% for testing:\n        \n```{r, echo=T, message=FALSE, warning=FALSE, cache = FALSE, results=\"markup\"}\ninTrain <- createDataPartition(y=training$classe, p=.75, list=FALSE)\ntesting <- training[-inTrain, ]\ntraining <- training[inTrain, ]\n```\n\n###Model Preprocessing and Cross Validation\nAll the predictors were plotted to identify any extreme values. An example is shown below. 9 row values were removed from the dataset.\n\n```{r, echo=T, message=FALSE, warning=FALSE, cache = FALSE, fig.width=6, fig.height=3.5, fig.align='center', results=\"markup\"}\n# Explore plots to determine outliers\npar(mfrow = c(1, 2))\nfor (i in 1:(ncol(training)-1)) {\n\n    if (names(training[i]) == \"accel_belt_y\") {\n        hist(training[[i]], xlab = names(training[i]), main = names(training[i]))\n        plot(training[[i]], main = names(training[i]))\n     }\n}\n\n#Outliers values are removed\nout.rows <- training$accel_belt_y > 100 | training$accel_belt_y < -50 |\n        training$total_accel_dumbbell > 50 | training$gyros_dumbbell_x < -50 |\n        training$gyros_dumbbell_y > 10 | training$gyros_dumbbell_z  > 10 |\n        training$magnet_dumbbell_y < -2000 | training$total_accel_forearm > 90 |\n        training$gyros_forearm_x < -10 | training$gyros_forearm_y > 50 |\n        training$gyros_forearm_z > 50 | training$accel_forearm_y > 800\ntraining <- training[!out.rows, ]\n```\n\nThe number of variables is reduced to 26 by using Principle Component Analysis (PCA).  PCA is used to avoid overfitting by combining highly correlated fields as well as reduce the computational requirements.\n\n```{r, echo=F, message=FALSE, warning=FALSE, cache = TRUE, results=\"markup\"}\n#Shows how much pre-processing reduces the number of variables\npreObj <- preProcess(x = training, method = \"pca\")\nprint(preObj)\n```\n\nThe introduction of a training control process performs the PCA reduction and adds a 7 fold cross validation. 7-fold cross validation avoids overfitting by breaking the training set into 7 subsets.\n\n```{r, echo=T, message=FALSE, warning=FALSE, cache = TRUE, results=\"markup\"}\ntc <- trainControl(method = \"cv\", number = 7, verboseIter=FALSE , preProcOptions=\"pca\", allowParallel=TRUE)\n```\n\n```{r, echo=F, message=FALSE, warning=FALSE, cache = TRUE, results=\"hide\"}\nrm(testFile, testFilePath, testUrl, trainFile, trainFilePath, trainUrl, preObj, i, \n   out.rows, inTrain, nearzero, toberem, featured.cols)\n```\n\n###Model Selection\nThree base models to cover a wide variety of possible distributions are used: Random Forest, Gradient Boosting, and Linear Discriminated Analysis model. \n\n```{r, echo=T, message=FALSE, warning=FALSE, cache = TRUE, results=\"hide\"}\nrf.model <- suppressMessages(train(classe ~ ., data = training, method = \"rf\", trControl= tc))\ngbm.model <- suppressMessages(train(classe ~., data = training, method=\"gbm\", trControl = tc))\nlda.model <- suppressMessages(train(classe ~ ., data = training, method = \"lda\", trControl= tc))\n```\n\n```{r, echo=F, message=FALSE, warning=FALSE, cache = FALSE, results=\"hide\"}\nmodel <- c(\"Random Forest\", \"Gradient Boosting\", \"Linear Discriminate Analysis\" )\nBest_Accuracy <- c(\n        max(rf.model$results$Accuracy),\n        max(gbm.model$results$Accuracy),\n        max(lda.model$results$Accuracy)\n        )\nWorst_Accuracy_SD <- c(\n        max(rf.model$results$AccuracySD),\n        max(gbm.model$results$AccuracySD),\n        max(lda.model$results$AccuracySD)\n        )\nWorst_Accuracy <- c(\n        min(rf.model$results$Accuracy),\n        min(gbm.model$results$Accuracy),\n        min(lda.model$results$Accuracy)\n        )\nBest_Accuracy_SD <- c(\n        min(rf.model$results$AccuracySD),\n        min(gbm.model$results$AccuracySD),\n        min(lda.model$results$AccuracySD)\n        )\n```\n\nAccuracy comparision and out of sample errors:\n```{r, echo=F, message=FALSE, warning=FALSE, cache = FALSE, results=\"asis\"}\nperformance <- cbind(model, Best_Accuracy, Best_Accuracy_SD, Worst_Accuracy, Worst_Accuracy_SD)\nknitr::kable(performance)\n```\n\nBest Model:\n```{r, echo=F, message=FALSE, warning=FALSE, cache = TRUE, results=\"markup\"}\nrf.model\n```\n\nThe random forest model method was the best model with the average accuracy of each of the 7 folds greater than 98.5%.  In addition, the out-of-sample error was very low as the worst standard deviation average of the 7 folds was less than .35%.  \n\n###Prediction of “classe” variable for the test set\n\n```{r, echo=T, message=FALSE, warning=FALSE, cache = TRUE, results=\"hide\"}\nrf.Pred <- predict(rf.model, testing)\n```\n\nChecking our Predictions with confusionMatrix:\n\n```{r, echo=T, message=FALSE, warning=FALSE, cache = TRUE, results=\"markup\"}\nconfusionMatrix(rf.Pred, testing$classe)\n```\n\n##Conclusions\n```{r, echo=F, message=FALSE, warning=FALSE, cache = TRUE, results=\"hide\"}\nrf.Pred.test <- predict(rf.model, test)\n```\nThe random forest model provides an outstanding accuracy and, accordingly, the predictions for the test set were correct in 100% of the cases.\n",
    "created" : 1454219223849.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "538333590",
    "id" : "AA3E3B0F",
    "lastKnownWriteTime" : 1454220923,
    "path" : "~/GitHub/machinelearning1/PredictiveBarbellLifts.rmd",
    "project_path" : "PredictiveBarbellLifts.rmd",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "type" : "r_markdown"
}